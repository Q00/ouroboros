# Ouroboros Example App: Reddit Second Brain

## Self-Evolving Knowledge Curator

**App Version**: 1.1  
**Ouroboros Version**: 0.4+  
**Use Case**: 24/7 Reddit í¬ë¡¤ë§ â†’ ê°œì¸í™”ëœ ì„¸ì»¨ë“œ ë¸Œë ˆì¸ êµ¬ì¶•

---

## Table of Contents

1. [Overview](#1-overview)
2. [Architecture](#2-architecture)
3. [Seed Definition](#3-seed-definition)
4. [Tools Specification](#4-tools-specification)
5. [Execution Flow](#5-execution-flow)
6. [Self-Improvement Mechanisms](#6-self-improvement-mechanisms)
7. [Configuration](#7-configuration)
8. [Implementation](#8-implementation)
9. [Cost Analysis](#9-cost-analysis)
10. [Deployment](#10-deployment)

---

## 1. Overview

### 1.1 What It Does

Reddit Second Brainì€ Ouroboros í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬:

1. **24ì‹œê°„ ë¬´ì¤‘ë‹¨**ìœ¼ë¡œ ì§€ì •ëœ ì„œë¸Œë ˆë”§ì„ í¬ë¡¤ë§
2. **Tier 0 ì‚¬ì „ í•„í„°**ë¡œ 70% ë¹„ìš© ì ˆê° â­ v1.1
3. **ê°œì¸ ê´€ì‹¬ì‚¬**ì— ë§ëŠ” ê¸€ë§Œ í•„í„°ë§
4. **ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ** í›„ ì„¸ì»¨ë“œ ë¸Œë ˆì¸(Obsidian)ì— ì €ì¥
5. **Implicit Feedback**ìœ¼ë¡œ íŒŒì¼ ì •ë¦¬ë§Œìœ¼ë¡œ ìë™ í•™ìŠµ â­ v1.1
6. **ì˜¨í†¨ë¡œì§€ ì§„í™”**ë¡œ ìƒˆë¡œìš´ ê´€ì‹¬ì‚¬ ìë™ ë°œê²¬

### 1.2 v1.1 Key Improvements

| Feature | v1.0 | v1.1 |
|---------|------|------|
| **Pre-filtering** | LLM only ($300/month) | Tier 0 embedding ($50-193/month) |
| **Feedback** | CLI ì…ë ¥ í•„ìš” | íŒŒì¼ ì´ë™/ì‚­ì œë¡œ ìë™ ìˆ˜ì§‘ |
| **UX Friction** | High | Zero |
| **Cost** | $300/month | $50-193/month |

### 1.3 Why Ouroboros?

| ì¼ë°˜ í¬ë¡¤ëŸ¬ | Ouroboros ê¸°ë°˜ |
|------------|---------------|
| ì •ì  ê·œì¹™ ê¸°ë°˜ í•„í„°ë§ | í•™ìŠµí•˜ëŠ” ì˜¨í†¨ë¡œì§€ |
| ì˜¤ë¥˜ ì‹œ ì¤‘ë‹¨ | Lateral Thinkingìœ¼ë¡œ ìš°íšŒ |
| ì¼ì • ë¹„ìš© | Tier 0 (0x) ~ Frontier (30x) ë™ì  ë°°ë¶„ |
| ìˆ˜ë™ í”¼ë“œë°± ì…ë ¥ | Implicit Feedback (Zero Friction) |

### 1.4 Core Value

```
"Obsidianì—ì„œ í‰ì†Œì²˜ëŸ¼ íŒŒì¼ë§Œ ì •ë¦¬í•˜ë©´,
 ì‹œìŠ¤í…œì´ ì•Œì•„ì„œ ë‚´ ê´€ì‹¬ì‚¬ë¥¼ í•™ìŠµí•˜ê³  ì§„í™”í•œë‹¤."
```

---

## 2. Architecture

### 2.1 System Diagram

```mermaid
graph TD
    subgraph "Data Sources"
        R1[r/MachineLearning]
        R2[r/LocalLLaMA]
        R3[r/ExperiencedDevs]
        R4[r/SideProject]
        RN[r/...]
    end

    subgraph "Ouroboros Engine"
        Crawler[Reddit Crawler\nFrugal 1x]
        Filter[Relevance Filter\nStandard 10x]
        Analyzer[Insight Extractor\nStandard 10x]
        
        Crawler --> Filter
        Filter --> Analyzer
    end

    subgraph "Storage Layer"
        Brain[(Second Brain\nObsidian/Notion)]
        Ontology[(Interest\nOntology)]
        Feedback[(User\nFeedback)]
    end

    subgraph "Evolution Loop"
        Retro[Daily Retrospective]
        Consensus{Ontology\nChange?}
    end

    R1 & R2 & R3 & R4 & RN --> Crawler
    Analyzer --> Brain
    Analyzer --> Ontology
    
    Brain --> Feedback
    Feedback --> Retro
    Retro --> Consensus
    Consensus -->|Yes, 30x| Ontology
    Ontology --> Filter
```

### 2.2 Component Mapping

| Ouroboros Component | Reddit Brain ì ìš© |
|--------------------|------------------|
| **Seed** | ì´ˆê¸° ê´€ì‹¬ì‚¬, ì„œë¸Œë ˆë”§ ëª©ë¡, í’ˆì§ˆ ê¸°ì¤€ |
| **PAL Router** | í¬ë¡¤ë§(1x), í•„í„°ë§(10x), í•©ì˜(30x) |
| **Double Diamond** | ìˆ˜ì§‘â†’í•„í„°â†’ë¶„ì„â†’ì €ì¥ ì‚¬ì´í´ |
| **Stagnation Detection** | ê°™ì€ íŒ¨í„´ ë°˜ë³µ ê°ì§€ |
| **Lateral Thinking** | API ì‹¤íŒ¨ ì‹œ ìš°íšŒ, ìƒˆ ì„œë¸Œë ˆë”§ íƒìƒ‰ |
| **Consensus** | ì˜¨í†¨ë¡œì§€ ë³€ê²½ ìŠ¹ì¸ |
| **Secondary Loop** | "ë‚˜ì¤‘ì— ì½ì„ ê¸€" ì¶•ì  |
| **Retrospective** | ì¼ì¼ í•™ìŠµ ë° ê°œì„  |

---

## 3. Seed Definition

### 3.1 Complete Seed YAML

```yaml
# seeds/reddit-brain-v1.yaml

seed:
  id: "reddit-brain-v1"
  created_at: "2026-01-12"
  ambiguity_score: 0.15  # â‰¤ 0.2 requirement met
  version: "1.0"
  
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Goal Definition
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  goal:
    statement: |
      Redditì—ì„œ AI/ê°œë°œ/ìƒì‚°ì„± ê´€ë ¨ ê³ í’ˆì§ˆ ì •ë³´ë¥¼ 24ì‹œê°„ ìˆ˜ì§‘í•˜ì—¬
      ê°œì¸í™”ëœ ì„¸ì»¨ë“œ ë¸Œë ˆì¸ì„ êµ¬ì¶•í•œë‹¤.
      ì‹œìŠ¤í…œì€ ì‚¬ìš©ì í”¼ë“œë°±ì„ í•™ìŠµí•˜ì—¬ ì ì§„ì ìœ¼ë¡œ 
      ë” ì •í™•í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë§Œ í•„í„°ë§í•œë‹¤.
      
    success_metrics:
      - name: "daily_useful_posts"
        description: "í•˜ë£¨ ìœ ìš©í•œ ê¸€ ìˆ˜ì§‘ëŸ‰"
        target: 10
        measurement: "posts marked as useful by user"
        verification: "automated"
        
      - name: "precision"
        description: "ìˆ˜ì§‘ ì •í™•ë„"
        target: 0.8
        measurement: "useful_posts / total_collected"
        verification: "automated"
        
      - name: "recall_improvement"
        description: "ë†“ì¹œ ì¢‹ì€ ê¸€ ê°ì†Œìœ¨"
        target: "10% monthly improvement"
        measurement: "user manually added posts"
        verification: "manual"
        
      - name: "ontology_freshness"
        description: "ê´€ì‹¬ì‚¬ ì—…ë°ì´íŠ¸ ì£¼ê¸°"
        target: "weekly"
        measurement: "ontology modification events"
        verification: "automated"
        
    scope:
      includes:
        - "Reddit public posts"
        - "Post titles, content, comments (top 5)"
        - "User-defined subreddits"
        - "Cross-post discovery"
      excludes:
        - "Private subreddits"
        - "NSFW content"
        - "Deleted posts"
        - "Full comment threads"

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Constraints
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  constraints:
    hard:
      - id: "rate_limit"
        description: "Reddit API rate limit compliance"
        rule: "max 100 requests per minute"
        
      - id: "no_duplicates"
        description: "No duplicate content"
        rule: "check post_id before storage"
        
      - id: "data_retention"
        description: "Respect Reddit ToS"
        rule: "no bulk data resale"
        
    soft:
      - id: "upvote_threshold"
        description: "Prefer quality posts"
        rule: "prefer posts with score >= 10"
        weight: 0.7
        
      - id: "recency"
        description: "Prefer recent content"
        rule: "prefer posts < 7 days old"
        weight: 0.8
        
      - id: "language"
        description: "Language preference"
        rule: "English preferred, Korean accepted"
        weight: 0.6

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Initial Ontology (Will Evolve)
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ontology:
    version: "1.0"
    
    interests:
      primary:
        - concept: "AI Agents"
          keywords: ["autonomous agent", "AI agent", "agentic", "multi-agent"]
          weight: 1.0
          
        - concept: "LLM Applications"
          keywords: ["LLM", "GPT", "Claude", "fine-tuning", "RAG", "prompt engineering"]
          weight: 1.0
          
        - concept: "System Design"
          keywords: ["architecture", "scalability", "distributed", "microservices"]
          weight: 0.9
          
        - concept: "Developer Productivity"
          keywords: ["workflow", "automation", "CLI tools", "dev tools"]
          weight: 0.8
          
      secondary:
        - concept: "Startup Insights"
          keywords: ["founder", "startup", "MVP", "product market fit"]
          weight: 0.6
          
        - concept: "Career Growth"
          keywords: ["senior engineer", "staff engineer", "tech lead", "career"]
          weight: 0.5
          
    exclude:
      explicit:
        - pattern: "meme"
          reason: "Low signal"
        - pattern: "job posting"
          reason: "Not learning content"
        - pattern: "hiring"
          reason: "Not learning content"
        - pattern: "salary"
          reason: "Not primary interest"
          
      learned: []  # Will be populated by feedback
      
    subreddits:
      active:
        - name: "MachineLearning"
          priority: 1
          scan_frequency: "hourly"
          
        - name: "LocalLLaMA"
          priority: 1
          scan_frequency: "hourly"
          
        - name: "ChatGPT"
          priority: 2
          scan_frequency: "2h"
          
        - name: "ExperiencedDevs"
          priority: 2
          scan_frequency: "2h"
          
        - name: "SideProject"
          priority: 3
          scan_frequency: "4h"
          
      candidates: []  # Discovered but not yet activated
      
    connections:
      # Concept relationships for graph building
      - from: "AI Agents"
        to: "LLM Applications"
        relation: "uses"
        
      - from: "System Design"
        to: "AI Agents"
        relation: "enables"

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Evaluation Principles
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  evaluation:
    relevance_criteria:
      - name: "novelty"
        question: "Does it teach something I don't know?"
        weight: 0.3
        
      - name: "actionability"
        question: "Can I apply this to my work?"
        weight: 0.3
        
      - name: "depth"
        question: "Is this surface-level or insightful?"
        weight: 0.2
        
      - name: "credibility"
        question: "Is the source trustworthy?"
        weight: 0.2
        
    quality_signals:
      positive:
        - "Code examples included"
        - "Personal experience shared"
        - "Data or benchmarks provided"
        - "Contrarian but well-argued view"
        
      negative:
        - "Pure opinion without evidence"
        - "Promotional content"
        - "Outdated information"
        - "Clickbait title"
        
    feedback_loop:
      positive_action: "User marks as useful"
      positive_effect: "Strengthen related interest weights"
      
      negative_action: "User dismisses"
      negative_effect: "Weaken or add to exclude list"
      
      threshold_for_ontology_change: 5  # N consistent signals

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Exit Conditions
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  exit_conditions:
    success:
      - condition: "User manually stops"
        action: "graceful_shutdown"
        
    failure:
      - condition: "Reddit API permanently blocked"
        action: "notify_and_pause"
        
      - condition: "Storage full"
        action: "archive_old_and_continue"
        
    timeout:
      max_runtime: "unlimited"  # 24/7 operation
      checkpoint_interval: "1h"
```

### 3.2 Seed Validation

```python
# Ambiguity check passes:
# - Success metrics: All measurable âœ“
# - Constraints: Clear rules âœ“
# - Scope: Well-defined boundaries âœ“
# - Technical: Specific APIs and tools âœ“
# â†’ Ambiguity Score: 0.15 â‰¤ 0.2 âœ“
```

---

## 4. Tools Specification

### 4.1 Tool Registry

| Tool | Tier | Cost | Purpose |
|------|------|------|---------|
| `reddit_crawler` | Frugal | 1x | ì„œë¸Œë ˆë”§ í¬ë¡¤ë§ |
| `embedding_prefilter` | **Zero** | **0x** | ë¡œì»¬ ì„ë² ë”© ì‚¬ì „ í•„í„° â­ NEW |
| `relevance_filter` | Standard | 10x | LLM ê´€ë ¨ì„± íŒë‹¨ |
| `insight_extractor` | Standard | 10x | ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ |
| `brain_writer` | Frugal | 1x | ì„¸ì»¨ë“œ ë¸Œë ˆì¸ ì €ì¥ |
| `implicit_feedback` | **Zero** | **0x** | íŒŒì¼ ê°ì‹œ ìë™ í”¼ë“œë°± â­ NEW |
| `ontology_updater` | Frontier | 30x | ì˜¨í†¨ë¡œì§€ ë³€ê²½ (Consensus) |

### 4.2 Tier 0: Embedding Pre-Filter â­ NEW

**í•µì‹¬ ì•„ì´ë””ì–´**: LLM í˜¸ì¶œ ì „ì— ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ë¡œ 70% ì‚¬ì „ íƒˆë½ â†’ ë¹„ìš© 1/10 ì ˆê°

```python
# tools/embedding_prefilter.py

from ouroboros.tools import MCPTool, ToolResult
from ouroboros.routing import Tier
from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingPrefilterTool(MCPTool):
    """
    ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ LLM í˜¸ì¶œ ì „ ì‚¬ì „ í•„í„°ë§.
    
    Tier: ZERO (ë¹„ìš© $0) - ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš©
    
    íš¨ê³¼: 250ê°œ ê¸€ ì¤‘ 70%ë¥¼ LLM ì—†ì´ íƒˆë½ â†’ 75ê°œë§Œ Standard ëª¨ë¸ë¡œ ì „ë‹¬
    """
    
    name = "embedding_prefilter"
    description = "Pre-filter posts using local embedding similarity"
    tier = Tier.ZERO  # No LLM cost
    
    def __init__(self, config: dict):
        # ê²½ëŸ‰ ì„ë² ë”© ëª¨ë¸ (ë¡œì»¬ ì‹¤í–‰, ë¬´ë£Œ)
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.similarity_threshold = config.get("threshold", 0.3)
        
        # ì˜¨í†¨ë¡œì§€ ì„ë² ë”© ìºì‹œ
        self._ontology_embeddings = None
        
    async def execute(self, params: dict) -> ToolResult:
        """
        Args:
            params:
                posts: list[dict] - í¬ë¡¤ë§ëœ ê¸€ ëª©ë¡
                ontology: dict - í˜„ì¬ ì˜¨í†¨ë¡œì§€
                top_percent: float - ìƒìœ„ N% í†µê³¼ (default: 30%)
        
        Returns:
            ToolResult with pre-filtered posts
        """
        posts = params["posts"]
        ontology = params["ontology"]
        top_percent = params.get("top_percent", 0.30)
        
        # 1. ì˜¨í†¨ë¡œì§€ í‚¤ì›Œë“œë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ìºì‹œ)
        ontology_text = self._build_ontology_text(ontology)
        ontology_embedding = self.model.encode(ontology_text)
        
        # 2. ê° í¬ìŠ¤íŠ¸ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        scored_posts = []
        for post in posts:
            post_text = f"{post['title']} {post['content'][:500]}"
            post_embedding = self.model.encode(post_text)
            
            # Cosine similarity
            similarity = np.dot(ontology_embedding, post_embedding) / (
                np.linalg.norm(ontology_embedding) * np.linalg.norm(post_embedding)
            )
            
            scored_posts.append({
                "post": post,
                "embedding_score": float(similarity)
            })
        
        # 3. ìƒìœ„ N%ë§Œ í†µê³¼
        scored_posts.sort(key=lambda x: x["embedding_score"], reverse=True)
        cutoff_index = int(len(scored_posts) * top_percent)
        passed = scored_posts[:cutoff_index]
        discarded = scored_posts[cutoff_index:]
        
        return ToolResult(
            success=True,
            data={
                "passed_posts": [p["post"] for p in passed],
                "passed_scores": [p["embedding_score"] for p in passed],
                "input_count": len(posts),
                "output_count": len(passed),
                "discard_count": len(discarded),
                "filter_rate": 1 - top_percent
            },
            metadata={
                "model": "all-MiniLM-L6-v2",
                "threshold": self.similarity_threshold,
                "cost": 0  # $0 - local execution
            }
        )
    
    def _build_ontology_text(self, ontology: dict) -> str:
        """ì˜¨í†¨ë¡œì§€ë¥¼ ë‹¨ì¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        parts = []
        
        for interest in ontology.get("interests", {}).get("primary", []):
            parts.append(interest["concept"])
            parts.extend(interest.get("keywords", []))
            
        for interest in ontology.get("interests", {}).get("secondary", []):
            parts.append(interest["concept"])
            
        return " ".join(parts)
    
    def update_ontology_cache(self, ontology: dict):
        """ì˜¨í†¨ë¡œì§€ ë³€ê²½ ì‹œ ì„ë² ë”© ìºì‹œ ì—…ë°ì´íŠ¸"""
        ontology_text = self._build_ontology_text(ontology)
        self._ontology_embeddings = self.model.encode(ontology_text)
```

**ë¹„ìš© íš¨ê³¼**:
```
Before (Tier 0 ì—†ìŒ):
  250 posts Ã— Standard 10x = 2500x cost units

After (Tier 0 ì ìš©):
  250 posts Ã— Tier 0 (0x) = 0 cost
  75 posts (30%) Ã— Standard 10x = 750x cost units
  
  ì ˆê°: 70% ë¹„ìš© ê°ì†Œ
```

### 4.2 Tool Implementations

```python
# tools/reddit_crawler.py

from ouroboros.tools import MCPTool, ToolResult
from ouroboros.routing import Tier
import praw
from datetime import datetime, timedelta

class RedditCrawlerTool(MCPTool):
    """
    Reddit APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì§€ì •ëœ ì„œë¸Œë ˆë”§ì—ì„œ ìƒˆ ê¸€ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Tier: Frugal (1x cost) - API í˜¸ì¶œë§Œ, LLM ì‚¬ìš© ì—†ìŒ
    """
    
    name = "reddit_crawler"
    description = "Crawl new posts from specified subreddits"
    tier = Tier.FRUGAL
    
    def __init__(self, config: dict):
        self.reddit = praw.Reddit(
            client_id=config["client_id"],
            client_secret=config["client_secret"],
            user_agent=config["user_agent"]
        )
        self.seen_posts: set[str] = set()  # Deduplication
        
    async def execute(self, params: dict) -> ToolResult:
        """
        Args:
            params:
                subreddits: list[dict] - í¬ë¡¤ë§í•  ì„œë¸Œë ˆë”§ ëª©ë¡
                limit: int - ì„œë¸Œë ˆë”§ë‹¹ ìµœëŒ€ ê¸€ ìˆ˜
                min_score: int - ìµœì†Œ upvote ìˆ˜
                max_age_days: int - ìµœëŒ€ ê¸€ ë‚˜ì´
        
        Returns:
            ToolResult with list of posts
        """
        subreddits = params.get("subreddits", [])
        limit = params.get("limit", 50)
        min_score = params.get("min_score", 10)
        max_age = timedelta(days=params.get("max_age_days", 7))
        
        posts = []
        errors = []
        
        for sub_config in subreddits:
            sub_name = sub_config["name"]
            
            try:
                subreddit = self.reddit.subreddit(sub_name)
                
                for post in subreddit.new(limit=limit):
                    # Skip if already seen
                    if post.id in self.seen_posts:
                        continue
                        
                    # Apply soft constraints
                    post_age = datetime.utcnow() - datetime.utcfromtimestamp(post.created_utc)
                    if post_age > max_age:
                        continue
                        
                    if post.score < min_score:
                        continue
                    
                    # Collect post data
                    self.seen_posts.add(post.id)
                    posts.append({
                        "id": post.id,
                        "subreddit": sub_name,
                        "title": post.title,
                        "content": post.selftext[:2000],  # Truncate
                        "url": post.url,
                        "score": post.score,
                        "num_comments": post.num_comments,
                        "created_utc": post.created_utc,
                        "author": str(post.author),
                        "permalink": f"https://reddit.com{post.permalink}",
                        "top_comments": self._get_top_comments(post, limit=3)
                    })
                    
            except Exception as e:
                errors.append({"subreddit": sub_name, "error": str(e)})
                
        return ToolResult(
            success=len(errors) == 0,
            data={"posts": posts, "count": len(posts)},
            errors=errors if errors else None,
            metadata={"subreddits_scanned": len(subreddits)}
        )
    
    def _get_top_comments(self, post, limit: int = 3) -> list[str]:
        """ìƒìœ„ ëŒ“ê¸€ ì¶”ì¶œ"""
        comments = []
        post.comments.replace_more(limit=0)
        for comment in post.comments[:limit]:
            if hasattr(comment, 'body'):
                comments.append(comment.body[:500])
        return comments
```

```python
# tools/relevance_filter.py

from ouroboros.tools import MCPTool, ToolResult
from ouroboros.routing import Tier

class RelevanceFilterTool(MCPTool):
    """
    ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ìœ¼ë¡œ ê¸€ì˜ ê´€ë ¨ì„±ì„ íŒë‹¨í•©ë‹ˆë‹¤.
    
    Tier: Standard (10x cost) - LLM íŒë‹¨ í•„ìš”
    """
    
    name = "relevance_filter"
    description = "Filter posts based on ontology relevance"
    tier = Tier.STANDARD
    
    async def execute(self, params: dict) -> ToolResult:
        """
        Args:
            params:
                posts: list[dict] - í•„í„°ë§í•  ê¸€ ëª©ë¡
                ontology: dict - í˜„ì¬ ì˜¨í†¨ë¡œì§€
                threshold: float - ìµœì†Œ ê´€ë ¨ì„± ì ìˆ˜
        
        Returns:
            ToolResult with filtered posts and scores
        """
        posts = params["posts"]
        ontology = params["ontology"]
        threshold = params.get("threshold", 0.6)
        
        results = []
        
        for post in posts:
            # LLMì—ê²Œ ê´€ë ¨ì„± íŒë‹¨ ìš”ì²­
            relevance = await self._judge_relevance(post, ontology)
            
            if relevance["score"] >= threshold:
                results.append({
                    "post": post,
                    "relevance_score": relevance["score"],
                    "matched_interests": relevance["matched_interests"],
                    "reasoning": relevance["reasoning"]
                })
        
        # Sort by relevance score
        results.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        return ToolResult(
            success=True,
            data={
                "filtered_posts": results,
                "input_count": len(posts),
                "output_count": len(results),
                "filter_rate": 1 - (len(results) / len(posts)) if posts else 0
            }
        )
    
    async def _judge_relevance(self, post: dict, ontology: dict) -> dict:
        """LLMì„ ì‚¬ìš©í•œ ê´€ë ¨ì„± íŒë‹¨"""
        
        prompt = f"""
        Evaluate the relevance of this Reddit post to the user's interests.
        
        POST:
        Title: {post['title']}
        Content: {post['content'][:1000]}
        Subreddit: r/{post['subreddit']}
        Score: {post['score']} upvotes
        
        USER INTERESTS (Primary - weight 1.0):
        {self._format_interests(ontology['interests']['primary'])}
        
        USER INTERESTS (Secondary - weight 0.6):
        {self._format_interests(ontology['interests'].get('secondary', []))}
        
        EXCLUDE TOPICS:
        {ontology.get('exclude', {}).get('explicit', [])}
        
        Respond in JSON:
        {{
            "score": 0.0-1.0,
            "matched_interests": ["interest1", "interest2"],
            "reasoning": "brief explanation",
            "is_excluded": false
        }}
        """
        
        response = await self.llm.generate(prompt, response_format="json")
        return response
    
    def _format_interests(self, interests: list) -> str:
        return "\n".join([
            f"- {i['concept']}: {', '.join(i['keywords'])}"
            for i in interests
        ])
```

```python
# tools/insight_extractor.py

from ouroboros.tools import MCPTool, ToolResult
from ouroboros.routing import Tier

class InsightExtractorTool(MCPTool):
    """
    ê¸€ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  êµ¬ì¡°í™”í•©ë‹ˆë‹¤.
    
    Tier: Standard (10x cost) - ì‹¬ì¸µ ë¶„ì„ í•„ìš”
    """
    
    name = "insight_extractor"
    description = "Extract and structure insights from posts"
    tier = Tier.STANDARD
    
    async def execute(self, params: dict) -> ToolResult:
        """
        Args:
            params:
                posts: list[dict] - ë¶„ì„í•  ê¸€ ëª©ë¡ (with relevance data)
                ontology: dict - íƒœê¹…ìš© ì˜¨í†¨ë¡œì§€
        
        Returns:
            ToolResult with structured insights
        """
        posts = params["posts"]
        ontology = params["ontology"]
        
        insights = []
        
        for item in posts:
            post = item["post"]
            
            extracted = await self._extract_insight(post, ontology)
            
            insights.append({
                "source": {
                    "id": post["id"],
                    "title": post["title"],
                    "url": post["permalink"],
                    "subreddit": post["subreddit"],
                    "score": post["score"]
                },
                "insight": extracted["main_insight"],
                "key_points": extracted["key_points"],
                "tags": extracted["tags"],
                "connections": extracted["connections"],
                "actionability": extracted["actionability"],
                "relevance_score": item["relevance_score"],
                "extracted_at": datetime.utcnow().isoformat()
            })
        
        return ToolResult(
            success=True,
            data={"insights": insights, "count": len(insights)}
        )
    
    async def _extract_insight(self, post: dict, ontology: dict) -> dict:
        prompt = f"""
        Extract structured insights from this Reddit post.
        
        POST:
        Title: {post['title']}
        Content: {post['content']}
        Top Comments: {post.get('top_comments', [])}
        
        AVAILABLE TAGS (from user's ontology):
        {[i['concept'] for i in ontology['interests']['primary']]}
        
        Extract and respond in JSON:
        {{
            "main_insight": "One sentence summary of the key insight",
            "key_points": ["point1", "point2", "point3"],
            "tags": ["tag1", "tag2"],
            "connections": ["related concept from ontology"],
            "actionability": {{
                "score": 0.0-1.0,
                "suggested_action": "What can user do with this?"
            }}
        }}
        """
        
        return await self.llm.generate(prompt, response_format="json")
```

```python
# tools/brain_writer.py

from ouroboros.tools import MCPTool, ToolResult
from ouroboros.routing import Tier
from pathlib import Path
import json

class BrainWriterTool(MCPTool):
    """
    ì„¸ì»¨ë“œ ë¸Œë ˆì¸(Obsidian/Notion)ì— ì¸ì‚¬ì´íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    
    Tier: Frugal (1x cost) - íŒŒì¼/API ì‘ì—…ë§Œ
    
    â­ v1.1: Inbox í´ë”ì— ì €ì¥ â†’ ì‚¬ìš©ì í–‰ë™ìœ¼ë¡œ ìë™ í”¼ë“œë°± ìˆ˜ì§‘
    """
    
    name = "brain_writer"
    description = "Write insights to second brain storage"
    tier = Tier.FRUGAL
    
    def __init__(self, config: dict):
        self.storage_type = config.get("type", "obsidian")
        self.vault_path = Path(config.get("vault_path", "./vault"))
        
        # v1.1: Inbox í´ë” êµ¬ì¡°
        self.inbox_path = self.vault_path / "Inbox"
        self.archive_path = self.vault_path / "Archive"
        self.trash_path = self.vault_path / "Trash"
        
        # í´ë” ìƒì„±
        for folder in [self.inbox_path, self.archive_path, self.trash_path]:
            folder.mkdir(parents=True, exist_ok=True)
        
    async def execute(self, params: dict) -> ToolResult:
        insights = params["insights"]
        written = []
        
        for insight in insights:
            # v1.1: Inboxì— ì €ì¥ (ì‚¬ìš©ìê°€ ì •ë¦¬í•˜ë©´ í”¼ë“œë°± ë°œìƒ)
            path = await self._write_to_inbox(insight)
            written.append({"id": insight["source"]["id"], "path": str(path)})
        
        return ToolResult(
            success=True,
            data={"written": written, "count": len(written)}
        )
    
    async def _write_to_inbox(self, insight: dict) -> Path:
        """Inbox í´ë”ì— ë§ˆí¬ë‹¤ìš´ ì €ì¥"""
        
        safe_title = "".join(c for c in insight["source"]["title"][:50] 
                           if c.isalnum() or c in " -_").strip()
        filename = f"{safe_title}.md"
        
        # v1.1: ë©”íƒ€ë°ì´í„°ì— í”¼ë“œë°± ì¶”ì  ì •ë³´ ì¶”ê°€
        content = f"""---
source: {insight["source"]["url"]}
subreddit: r/{insight["source"]["subreddit"]}
score: {insight["source"]["score"]}
tags: {insight["tags"]}
relevance: {insight["relevance_score"]}
extracted: {insight["extracted_at"]}
post_id: {insight["source"]["id"]}
status: inbox
feedback: pending
---

# {insight["source"]["title"]}

## ğŸ’¡ Main Insight

{insight["insight"]}

## ğŸ“ Key Points

{chr(10).join(f"- {point}" for point in insight["key_points"])}

## ğŸ·ï¸ Tags

{" ".join(f"[[{tag}]]" for tag in insight["tags"])}

## ğŸ”— Connections

{" ".join(f"[[{conn}]]" for conn in insight["connections"])}

## âœ… Actionability

**Score**: {insight["actionability"]["score"]:.1%}

{insight["actionability"]["suggested_action"]}

---

> ğŸ“Œ **Tip**: ìœ ìš©í•˜ë©´ `/Archive`ë¡œ ì´ë™, í•„ìš”ì—†ìœ¼ë©´ ì‚­ì œí•˜ì„¸ìš”. ìë™ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤!

*Source: [{insight["source"]["subreddit"]}]({insight["source"]["url"]})*
"""
        
        filepath = self.inbox_path / filename
        filepath.write_text(content, encoding="utf-8")
        
        return filepath
```

### 4.5 Implicit Feedback Loop â­ NEW (v1.1)

**í•µì‹¬ ì•„ì´ë””ì–´**: CLI ì…ë ¥ ëŒ€ì‹  **íŒŒì¼ ì´ë™/ì‚­ì œ í–‰ìœ„**ë¥¼ í”¼ë“œë°±ìœ¼ë¡œ ìë™ í•´ì„

```python
# tools/implicit_feedback.py

from ouroboros.tools import MCPTool, ToolResult
from ouroboros.routing import Tier
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from pathlib import Path
import frontmatter
import asyncio

class ImplicitFeedbackTool(MCPTool):
    """
    Obsidian íŒŒì¼ ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ë¥¼ ê°ì‹œí•˜ì—¬ ìë™ìœ¼ë¡œ í”¼ë“œë°±ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Tier: ZERO (ë¹„ìš© $0) - íŒŒì¼ ì‹œìŠ¤í…œ ê°ì‹œë§Œ
    
    í”¼ë“œë°± ê·œì¹™:
    - /Inbox â†’ /Archive ì´ë™: useful (+1)
    - /Inbox íŒŒì¼ ì‚­ì œ: dismiss (-1)
    - #favorite íƒœê·¸ ì¶”ê°€: super_like (+2)
    - 7ì¼ ì´ìƒ /Inboxì— ë°©ì¹˜: ignored (-0.5)
    """
    
    name = "implicit_feedback"
    description = "Collect feedback from file system events"
    tier = Tier.ZERO  # No LLM cost
    
    def __init__(self, config: dict):
        self.vault_path = Path(config["vault_path"])
        self.inbox_path = self.vault_path / "Inbox"
        self.archive_path = self.vault_path / "Archive"
        
        self.feedback_buffer: list[dict] = []
        self.observer = None
        
    def start_watching(self):
        """íŒŒì¼ ì‹œìŠ¤í…œ ê°ì‹œ ì‹œì‘"""
        
        event_handler = FeedbackEventHandler(self)
        self.observer = Observer()
        
        # Inbox í´ë” ê°ì‹œ (ì‚­ì œ, ì´ë™ ê°ì§€)
        self.observer.schedule(event_handler, str(self.inbox_path), recursive=False)
        
        # Archive í´ë” ê°ì‹œ (íƒœê·¸ ë³€ê²½ ê°ì§€)
        self.observer.schedule(event_handler, str(self.archive_path), recursive=True)
        
        self.observer.start()
        
    def stop_watching(self):
        """íŒŒì¼ ì‹œìŠ¤í…œ ê°ì‹œ ì¤‘ì§€"""
        if self.observer:
            self.observer.stop()
            self.observer.join()
    
    async def execute(self, params: dict) -> ToolResult:
        """ì¶•ì ëœ í”¼ë“œë°± ë°˜í™˜ ë° ë²„í¼ í´ë¦¬ì–´"""
        
        feedback = self.feedback_buffer.copy()
        self.feedback_buffer.clear()
        
        return ToolResult(
            success=True,
            data={
                "feedback": feedback,
                "count": len(feedback)
            }
        )
    
    def record_feedback(self, post_id: str, action: str, score: float, context: dict = None):
        """í”¼ë“œë°± ê¸°ë¡"""
        self.feedback_buffer.append({
            "post_id": post_id,
            "action": action,
            "score": score,
            "timestamp": datetime.utcnow().isoformat(),
            "context": context or {}
        })


class FeedbackEventHandler(FileSystemEventHandler):
    """íŒŒì¼ ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ë¥¼ í”¼ë“œë°±ìœ¼ë¡œ ë³€í™˜"""
    
    def __init__(self, feedback_tool: ImplicitFeedbackTool):
        self.tool = feedback_tool
        
    def on_moved(self, event):
        """íŒŒì¼ ì´ë™ ê°ì§€"""
        if event.is_directory:
            return
            
        src = Path(event.src_path)
        dest = Path(event.dest_path)
        
        # Inbox â†’ Archive ì´ë™ = useful
        if "Inbox" in str(src) and "Archive" in str(dest):
            post_id = self._extract_post_id(src)
            if post_id:
                self.tool.record_feedback(
                    post_id=post_id,
                    action="useful",
                    score=1.0,
                    context={"moved_to": str(dest)}
                )
                
    def on_deleted(self, event):
        """íŒŒì¼ ì‚­ì œ ê°ì§€"""
        if event.is_directory:
            return
            
        src = Path(event.src_path)
        
        # Inboxì—ì„œ ì‚­ì œ = dismiss
        if "Inbox" in str(src):
            post_id = self._extract_post_id(src)
            if post_id:
                self.tool.record_feedback(
                    post_id=post_id,
                    action="dismiss",
                    score=-1.0
                )
                
    def on_modified(self, event):
        """íŒŒì¼ ìˆ˜ì • ê°ì§€ (íƒœê·¸ ë³€ê²½)"""
        if event.is_directory:
            return
            
        filepath = Path(event.src_path)
        if not filepath.suffix == ".md":
            return
            
        # #favorite íƒœê·¸ í™•ì¸
        try:
            post = frontmatter.load(filepath)
            tags = post.get("tags", [])
            
            if "favorite" in tags or "#favorite" in post.content:
                post_id = post.get("post_id")
                if post_id:
                    self.tool.record_feedback(
                        post_id=post_id,
                        action="super_like",
                        score=2.0,
                        context={"tags": tags}
                    )
        except:
            pass
            
    def _extract_post_id(self, filepath: Path) -> str | None:
        """ë§ˆí¬ë‹¤ìš´ í”„ë¡ íŠ¸ë§¤í„°ì—ì„œ post_id ì¶”ì¶œ"""
        try:
            post = frontmatter.load(filepath)
            return post.get("post_id")
        except:
            return None
```

**UX íë¦„**:
```
ì‚¬ìš©ì í–‰ë™                    â†’  ìë™ í”¼ë“œë°±
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
íŒŒì¼ì„ /Archiveë¡œ ì´ë™        â†’  useful (+1)
íŒŒì¼ ì‚­ì œ                      â†’  dismiss (-1)
#favorite íƒœê·¸ ì¶”ê°€           â†’  super_like (+2)
7ì¼ê°„ /Inboxì— ë°©ì¹˜           â†’  ignored (-0.5)
íŒŒì¼ ë‚´ìš©ì— ë©”ëª¨ ì¶”ê°€          â†’  engaged (+0.5)
```

**Zero Friction í•™ìŠµ**:
```
ê¸°ì¡´: CLIì—ì„œ `feedback post_id useful` ì…ë ¥ í•„ìš”
ê°œì„ : Obsidianì—ì„œ í‰ì†Œì²˜ëŸ¼ íŒŒì¼ ì •ë¦¬ë§Œ í•˜ë©´ ìë™ í•™ìŠµ!
```

```python
# tools/ontology_updater.py

from ouroboros.tools import MCPTool, ToolResult
from ouroboros.routing import Tier

class OntologyUpdaterTool(MCPTool):
    """
    ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ì˜¨í†¨ë¡œì§€ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    
    Tier: Frontier (30x cost) - Consensus í•„ìš”
    
    âš ï¸ ì´ ë„êµ¬ëŠ” Consensus Trigger Matrixì˜ 
       'ontology_change' ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """
    
    name = "ontology_updater"
    description = "Update interest ontology based on feedback"
    tier = Tier.FRONTIER
    requires_consensus = True
    
    async def execute(self, params: dict) -> ToolResult:
        """
        Args:
            params:
                current_ontology: dict - í˜„ì¬ ì˜¨í†¨ë¡œì§€
                feedback_data: list[dict] - ì¶•ì ëœ í”¼ë“œë°±
                threshold: int - ë³€ê²½ì— í•„ìš”í•œ ì‹ í˜¸ ìˆ˜
        
        Returns:
            ToolResult with proposed ontology changes
        """
        ontology = params["current_ontology"]
        feedback = params["feedback_data"]
        threshold = params.get("threshold", 5)
        
        # Analyze feedback patterns
        analysis = await self._analyze_feedback(feedback)
        
        # Generate change proposals
        proposals = []
        
        # New interests to add
        for pattern in analysis["emerging_interests"]:
            if pattern["count"] >= threshold:
                proposals.append({
                    "type": "add_interest",
                    "data": {
                        "concept": pattern["concept"],
                        "keywords": pattern["keywords"],
                        "weight": 0.5,  # Start with low weight
                        "evidence": pattern["examples"]
                    }
                })
        
        # Interests to strengthen
        for pattern in analysis["positive_patterns"]:
            if pattern["count"] >= threshold:
                proposals.append({
                    "type": "strengthen_interest",
                    "data": {
                        "concept": pattern["concept"],
                        "weight_delta": 0.1
                    }
                })
        
        # Topics to exclude
        for pattern in analysis["negative_patterns"]:
            if pattern["count"] >= threshold:
                proposals.append({
                    "type": "add_exclusion",
                    "data": {
                        "pattern": pattern["pattern"],
                        "reason": f"User dismissed {pattern['count']} times"
                    }
                })
        
        # New subreddits to consider
        for sub in analysis["suggested_subreddits"]:
            if sub["mentions"] >= threshold:
                proposals.append({
                    "type": "add_candidate_subreddit",
                    "data": {
                        "name": sub["name"],
                        "reason": sub["reason"]
                    }
                })
        
        return ToolResult(
            success=True,
            data={
                "proposals": proposals,
                "analysis_summary": analysis["summary"],
                "requires_consensus": len(proposals) > 0
            },
            metadata={
                "feedback_count": len(feedback),
                "proposal_count": len(proposals)
            }
        )
    
    async def _analyze_feedback(self, feedback: list) -> dict:
        """í”¼ë“œë°± íŒ¨í„´ ë¶„ì„"""
        
        prompt = f"""
        Analyze user feedback patterns to suggest ontology updates.
        
        FEEDBACK DATA ({len(feedback)} items):
        {json.dumps(feedback[:50], indent=2)}  # Sample
        
        Identify:
        1. Emerging interests (topics user consistently likes but not in ontology)
        2. Positive patterns (existing interests being reinforced)
        3. Negative patterns (topics user consistently dismisses)
        4. Suggested subreddits (mentioned in liked posts)
        
        Respond in JSON:
        {{
            "emerging_interests": [...],
            "positive_patterns": [...],
            "negative_patterns": [...],
            "suggested_subreddits": [...],
            "summary": "brief analysis"
        }}
        """
        
        return await self.llm.generate(prompt, response_format="json")
```

---

## 5. Execution Flow

### 5.1 Hourly Cycle (Double Diamond + Tier 0)

```yaml
# ë§¤ ì‹œê°„ ì‹¤í–‰ë˜ëŠ” ì‚¬ì´í´ (v1.1 ê°œì„ )

hourly_cycle:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # DISCOVER (Diverge) - ë„“ê²Œ ìˆ˜ì§‘
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  discover:
    tier: "frugal"  # 1x
    tool: "reddit_crawler"
    
    input:
      subreddits: "from seed.ontology.subreddits.active"
      limit: 50
      min_score: 10
      max_age_days: 7
      
    output:
      expected: "~250 posts"

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # PRE-FILTER (Tier 0) - ë¡œì»¬ ì„ë² ë”©ìœ¼ë¡œ 70% ì‚¬ì „ íƒˆë½ â­ NEW
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  prefilter:
    tier: "zero"  # 0x (FREE!)
    tool: "embedding_prefilter"
    
    input:
      posts: "from discover.output"
      ontology: "current ontology"
      top_percent: 0.30  # ìƒìœ„ 30%ë§Œ í†µê³¼
      
    output:
      expected: "~75 posts (70% filtered for FREE)"
      
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # DEFINE (Converge) - LLM ê´€ë ¨ì„± í•„í„°ë§
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  define:
    tier: "standard"  # 10x
    tool: "relevance_filter"
    
    input:
      posts: "from prefilter.output"  # 250 â†’ 75 (ì´ë¯¸ 70% ê°ì†Œ!)
      ontology: "current ontology"
      threshold: 0.6
      
    output:
      expected: "~25 posts (ì¶”ê°€ 66% filtered)"
      
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # DESIGN (Diverge) - ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  design:
    tier: "standard"  # 10x
    tool: "insight_extractor"
    
    input:
      posts: "from define.output"
      ontology: "for tagging"
      
    output:
      expected: "25 structured insights"
      
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # DELIVER (Converge) - Inboxì— ì €ì¥
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  deliver:
    tier: "frugal"  # 1x
    tool: "brain_writer"
    
    input:
      insights: "from design.output"
      target: "obsidian"
      destination: "/Inbox"  # v1.1: Inboxì— ì €ì¥
      
    output:
      expected: "25 markdown files in /Inbox"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Implicit Feedback (Background) â­ NEW
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
implicit_feedback:
  tool: "implicit_feedback"
  mode: "background_watcher"
  
  events:
    - trigger: "file_moved_to_archive"
      action: "record_useful"
      score: +1
      
    - trigger: "file_deleted"
      action: "record_dismiss"
      score: -1
      
    - trigger: "favorite_tag_added"
      action: "record_super_like"
      score: +2
      
    - trigger: "file_stale_7_days"
      action: "record_ignored"
      score: -0.5
```

### 5.2 Updated Architecture Diagram (v1.1)

```mermaid
graph TD
    subgraph "Phase 1: Collection (Tier 0 & 1)"
        Reddit[Reddit API] --> Crawler[Crawler\nFrugal 1x]
        Crawler --> Embed[Embedding Pre-Filter\nTier 0 / $0]
        Embed -- "Low Similarity\n70% discard" --> Trash[Discard]
        Embed -- "High Similarity\n30% pass" --> Filter[Relevance Filter\nStandard 10x]
    end

    subgraph "Phase 2: Knowledge (Standard)"
        Filter --> Analyzer[Insight Extractor\nStandard 10x]
        Analyzer --> Inbox[Obsidian /Inbox]
    end

    subgraph "Phase 3: Evolution (Implicit Feedback)"
        Inbox -- "Move to /Archive" --> Watcher[File Watcher\nTier 0 / $0]
        Inbox -- "Delete" --> Watcher
        Inbox -- "#favorite" --> Watcher
        Watcher --> Buffer[(Feedback\nBuffer)]
        Buffer --> Retro[Daily Retrospective\nStandard 10x]
        Retro -- "Pattern Found" --> Consensus{Consensus\nFrontier 30x}
        Consensus --> Ontology[(Ontology.yaml)]
        Ontology --> Embed & Filter
    end
```

### 5.2 Stagnation Scenarios

```yaml
stagnation_detection:
  scenarios:
    # ì‹œë‚˜ë¦¬ì˜¤ 1: ê°™ì€ ê¸€ë§Œ ë°˜ë³µ ìˆ˜ì§‘
    duplicate_content:
      pattern: "no_drift"
      detection: "new_posts_count < 5 for 3 consecutive hours"
      response:
        persona: "the_researcher"
        action: "Search for new relevant subreddits"
        
    # ì‹œë‚˜ë¦¬ì˜¤ 2: API ì—ëŸ¬ ë°˜ë³µ
    api_failure:
      pattern: "spinning"
      detection: "same API error 2+ times"
      response:
        persona: "the_hacker"
        action: "Use backup API or web scraping fallback"
        
    # ì‹œë‚˜ë¦¬ì˜¤ 3: ê´€ë ¨ì„± ì ìˆ˜ ì •ì²´
    relevance_plateau:
      pattern: "diminishing_returns"
      detection: "avg_relevance_score unchanged for 24h"
      response:
        persona: "the_contrarian"
        action: "Question current ontology assumptions"
```

---

## 6. Self-Improvement Mechanisms

### 6.1 Feedback Loop

```python
# ì‚¬ìš©ì í”¼ë“œë°± ì²˜ë¦¬ íë¦„

class FeedbackProcessor:
    """
    ì‚¬ìš©ì í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ê³  í•™ìŠµì— ë°˜ì˜í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, ontology_updater: OntologyUpdaterTool):
        self.feedback_buffer: list[dict] = []
        self.updater = ontology_updater
        
    async def record_feedback(
        self, 
        post_id: str, 
        action: Literal["useful", "dismiss", "save_for_later"]
    ):
        """
        ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë¡
        
        Actions:
        - useful: ìœ ìš©í•œ ê¸€ë¡œ í‘œì‹œ â†’ ê´€ë ¨ ê´€ì‹¬ì‚¬ ê°•í™”
        - dismiss: ê´€ì‹¬ ì—†ìŒ í‘œì‹œ â†’ íŒ¨í„´ í•™ìŠµí•˜ì—¬ ì œì™¸
        - save_for_later: TODO Registryë¡œ ì´ë™
        """
        self.feedback_buffer.append({
            "post_id": post_id,
            "action": action,
            "timestamp": datetime.utcnow().isoformat(),
            "context": await self._get_post_context(post_id)
        })
        
        # ë²„í¼ê°€ ì„ê³„ê°’ì— ë„ë‹¬í•˜ë©´ ë¶„ì„ íŠ¸ë¦¬ê±°
        if len(self.feedback_buffer) >= 20:
            await self._trigger_analysis()
    
    async def _trigger_analysis(self):
        """í”¼ë“œë°± ë¶„ì„ ë° ì˜¨í†¨ë¡œì§€ ì—…ë°ì´íŠ¸ ì œì•ˆ"""
        
        # Standard ëª¨ë¸ë¡œ ë¶„ì„
        analysis = await self.updater.execute({
            "current_ontology": self.current_ontology,
            "feedback_data": self.feedback_buffer,
            "threshold": 5
        })
        
        if analysis.data["requires_consensus"]:
            # Consensus í•„ìš” â†’ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤
            await self._request_ontology_consensus(
                analysis.data["proposals"]
            )
        
        self.feedback_buffer.clear()
```

### 6.2 Ontology Evolution Example

```yaml
# Week 1: ì´ˆê¸° ì˜¨í†¨ë¡œì§€
ontology_v1:
  interests:
    primary:
      - concept: "AI Agents"
        weight: 1.0
      - concept: "LLM Applications"
        weight: 1.0
        
# Week 2: í”¼ë“œë°± ë°˜ì˜
ontology_v2:
  interests:
    primary:
      - concept: "AI Agents"
        weight: 1.0
      - concept: "LLM Applications"
        weight: 1.0
      - concept: "MCP Tools"        # âœ¨ NEW: ìì£¼ ì €ì¥í•œ íŒ¨í„´
        weight: 0.7
        
    learned_keywords:
      - "Claude API"               # âœ¨ ê¸ì • í”¼ë“œë°±ì—ì„œ ì¶”ì¶œ
      - "function calling"
      
  exclude:
    learned:
      - pattern: "GPT wrapper"     # âœ¨ ìì£¼ dismissí•œ íŒ¨í„´
        reason: "User dismissed 7 times"
        
# Week 4: ì„œë¸Œë ˆë”§ í™•ì¥
ontology_v4:
  subreddits:
    active:
      - name: "MachineLearning"
      - name: "LocalLLaMA"
      - name: "ClaudeAI"           # âœ¨ NEW: ê´€ë ¨ ê¸€ì—ì„œ ë°œê²¬
        priority: 2
        
    discovered:
      - name: "AnthropicAI"
        mentions: 12
        status: "candidate"
```

### 6.3 Daily Retrospective

```yaml
retrospective:
  schedule: "0 0 * * *"  # ë§¤ì¼ ìì •
  tier: "standard"       # 10x
  
  analysis:
    - name: "collection_quality"
      metrics:
        - "posts_collected_today"
        - "posts_marked_useful"
        - "posts_dismissed"
        - "precision_rate"
        
    - name: "drift_check"
      question: "Are we still aligned with original goal?"
      threshold: 0.3
      
    - name: "coverage_check"
      question: "Are there topics we're missing?"
      method: "analyze dismissed posts for patterns"
      
  outputs:
    - type: "daily_report"
      destination: "obsidian://Daily Notes"
      
    - type: "ontology_proposals"
      condition: "if significant patterns detected"
      action: "queue for consensus"
      
    - type: "subreddit_suggestions"
      condition: "if new sources discovered"
      action: "add to candidates"
```

---

## 7. Configuration

### 7.1 App-Specific Config

```yaml
# config/reddit-brain.yaml

app:
  name: "reddit-second-brain"
  version: "1.0.0"
  
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Reddit API Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
reddit:
  client_id: "${REDDIT_CLIENT_ID}"
  client_secret: "${REDDIT_CLIENT_SECRET}"
  user_agent: "ouroboros-reddit-brain/1.0"
  
  rate_limit:
    requests_per_minute: 60  # Conservative (API allows 100)
    
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Storage Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
storage:
  primary:
    type: "obsidian"
    vault_path: "/Users/me/SecondBrain"
    folder: "Reddit Insights"
    
  backup:
    type: "json"
    path: "./data/backup"
    
  feedback:
    type: "sqlite"
    path: "./data/feedback.db"
    
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Schedule Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
schedule:
  crawl_cycle:
    interval: "1h"
    
  retrospective:
    cron: "0 0 * * *"  # Daily at midnight
    
  checkpoint:
    interval: "30m"
    
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Thresholds
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
thresholds:
  relevance:
    minimum: 0.6
    
  feedback:
    ontology_update_trigger: 5  # N consistent signals
    
  stagnation:
    no_new_posts_hours: 3
    relevance_plateau_hours: 24
    
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Feature Flags
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
features:
  cross_post_discovery: true
  comment_analysis: true
  author_tracking: false  # Future
  sentiment_analysis: false  # Future
```

### 7.2 Ouroboros Integration Config

```yaml
# config/ouroboros.yaml (app-specific overrides)

# Import base config
extends: "ouroboros-config-v0.4.yaml"

# Override for Reddit Brain app
overrides:
  economics:
    # Budget constraints
    daily_budget:
      frugal_calls: 1000
      standard_calls: 200
      frontier_calls: 10
      
  evaluation:
    # Simplified for content curation
    stage_1_mechanical:
      checks:
        - duplicate_check  # Only this for crawled content
        
  consensus:
    triggers:
      mandatory:
        - "ontology_change"  # Main use case
      never:
        - "routine_ac_evaluation"  # Skip for hourly cycles
```

---

## 8. Implementation

### 8.1 Main Entry Point

```python
# main.py (v1.1)

from ouroboros import Ouroboros
from ouroboros.tools import ToolRegistry
from pathlib import Path

from tools.reddit_crawler import RedditCrawlerTool
from tools.embedding_prefilter import EmbeddingPrefilterTool  # â­ NEW
from tools.relevance_filter import RelevanceFilterTool
from tools.insight_extractor import InsightExtractorTool
from tools.brain_writer import BrainWriterTool
from tools.implicit_feedback import ImplicitFeedbackTool  # â­ NEW
from tools.ontology_updater import OntologyUpdaterTool

def create_app() -> Ouroboros:
    """Reddit Second Brain ì•± ìƒì„± (v1.1)"""
    
    # 1. Load seed
    app = Ouroboros.from_seed("seeds/reddit-brain-v1.yaml")
    
    # 2. Load config
    app.load_config("config/reddit-brain.yaml")
    
    # 3. Register tools
    reddit_config = app.config["reddit"]
    storage_config = app.config["storage"]
    
    app.register_tools([
        RedditCrawlerTool(reddit_config),
        EmbeddingPrefilterTool({"threshold": 0.3}),  # â­ Tier 0
        RelevanceFilterTool(),
        InsightExtractorTool(),
        BrainWriterTool(storage_config["primary"]),
        ImplicitFeedbackTool(storage_config["primary"]),  # â­ File watcher
        OntologyUpdaterTool()
    ])
    
    # 4. Start implicit feedback watcher â­ NEW
    feedback_tool = app.get_tool("implicit_feedback")
    feedback_tool.start_watching()
    
    # 5. Setup schedules
    app.schedule(
        "hourly_cycle",
        cron="0 * * * *",
        handler=hourly_cycle_handler
    )
    
    app.schedule(
        "daily_retrospective",
        cron="0 0 * * *",
        handler=retrospective_handler
    )
    
    return app


async def hourly_cycle_handler(app: Ouroboros):
    """ë§¤ì‹œê°„ í¬ë¡¤ë§ ì‚¬ì´í´ (v1.1)"""
    
    # DISCOVER
    crawl_result = await app.execute_tool(
        "reddit_crawler",
        {
            "subreddits": app.seed.ontology["subreddits"]["active"],
            "limit": 50,
            "min_score": 10
        }
    )
    
    if not crawl_result.success:
        return
    
    # PRE-FILTER (Tier 0) â­ NEW - 70% ë¬´ë£Œ íƒˆë½
    prefilter_result = await app.execute_tool(
        "embedding_prefilter",
        {
            "posts": crawl_result.data["posts"],
            "ontology": app.current_ontology,
            "top_percent": 0.30
        }
    )
    
    # DEFINE (ì´ì œ 75ê°œë§Œ ì²˜ë¦¬, was 250)
    filter_result = await app.execute_tool(
        "relevance_filter",
        {
            "posts": prefilter_result.data["passed_posts"],
            "ontology": app.current_ontology,
            "threshold": 0.6
        }
    )
    
    # DESIGN
    insights = await app.execute_tool(
        "insight_extractor",
        {
            "posts": filter_result.data["filtered_posts"],
            "ontology": app.current_ontology
        }
    )
    
    # DELIVER (to /Inbox)
    await app.execute_tool(
        "brain_writer",
        {
            "insights": insights.data["insights"],
            "target": "obsidian"
        }
    )
    
    # Log metrics
    app.log_metrics({
        "posts_crawled": crawl_result.data["count"],
        "posts_prefiltered": prefilter_result.data["output_count"],  # â­ NEW
        "posts_llm_filtered": filter_result.data["output_count"],
        "insights_saved": len(insights.data["insights"]),
        "tier0_savings": prefilter_result.data["discard_count"]  # â­ NEW
    })


async def retrospective_handler(app: Ouroboros):
    """ì¼ì¼ íšŒê³  ë° ê°œì„  (v1.1)"""
    
    # Collect implicit feedback â­ NEW
    feedback_tool = app.get_tool("implicit_feedback")
    feedback_result = await feedback_tool.execute({})
    
    # Trigger retrospective with collected feedback
    await app.run_retrospective(
        feedback_data=feedback_result.data["feedback"],  # â­ Auto-collected!
        analyze_feedback=True,
        propose_ontology_changes=True,
        generate_daily_report=True
    )


if __name__ == "__main__":
    app = create_app()
    
    print("ğŸ Starting Reddit Second Brain v1.1...")
    print("   âœ… Tier 0 embedding pre-filter enabled")
    print("   âœ… Implicit feedback watcher started")
    print("   âœ… Obsidian vault watching: /Inbox, /Archive")
    
    # Run 24/7
    app.run(
        mode="continuous",
        checkpoint_on_interrupt=True
    )
```

### 8.2 CLI Interface

```python
# cli.py

import click
from main import create_app

@click.group()
def cli():
    """Reddit Second Brain - Ouroboros App"""
    pass

@cli.command()
def start():
    """Start the 24/7 crawler"""
    app = create_app()
    click.echo("ğŸ Starting Reddit Second Brain...")
    app.run(mode="continuous")

@cli.command()
def cycle():
    """Run a single crawl cycle"""
    app = create_app()
    click.echo("ğŸ”„ Running single cycle...")
    asyncio.run(hourly_cycle_handler(app))

@cli.command()
def status():
    """Show current status"""
    app = create_app()
    status = app.get_status()
    
    click.echo(f"""
    ğŸ“Š Reddit Second Brain Status
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Running: {status['running']}
    Uptime: {status['uptime']}
    
    Today's Stats:
    - Posts crawled: {status['today']['crawled']}
    - Posts saved: {status['today']['saved']}
    - Precision: {status['today']['precision']:.1%}
    
    Ontology:
    - Version: {status['ontology']['version']}
    - Interests: {len(status['ontology']['interests'])}
    - Subreddits: {len(status['ontology']['subreddits'])}
    """)

@cli.command()
@click.argument('post_id')
@click.argument('action', type=click.Choice(['useful', 'dismiss']))
def feedback(post_id: str, action: str):
    """Record feedback for a post"""
    app = create_app()
    asyncio.run(app.feedback_processor.record_feedback(post_id, action))
    click.echo(f"âœ… Recorded: {action} for {post_id}")

@cli.command()
def retrospective():
    """Run manual retrospective"""
    app = create_app()
    click.echo("ğŸ” Running retrospective...")
    asyncio.run(retrospective_handler(app))

if __name__ == "__main__":
    cli()
```

---

## 9. Cost Analysis

### 9.1 Per-Cycle Cost Breakdown (v1.1 Optimized)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HOURLY CYCLE COST ESTIMATE (v1.1 with Tier 0)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  DISCOVER (Frugal 1x)                                       â”‚
â”‚  â””â”€ Reddit API calls: $0 (free API)                         â”‚
â”‚  â””â”€ Subtotal: $0.00                                         â”‚
â”‚                                                             â”‚
â”‚  PRE-FILTER (Tier 0) â­ NEW                                  â”‚
â”‚  â””â”€ Local embedding model: $0                               â”‚
â”‚  â””â”€ 250 posts â†’ 75 posts (70% discarded FREE)               â”‚
â”‚  â””â”€ Subtotal: $0.00                                         â”‚
â”‚                                                             â”‚
â”‚  DEFINE (Standard 10x)                                      â”‚
â”‚  â””â”€ Posts to filter: 75 (was 250)                           â”‚
â”‚  â””â”€ Tokens: ~15K input (was 50K)                            â”‚
â”‚  â””â”€ Cost: 15K Ã— $0.003/1K = $0.045                          â”‚
â”‚  â””â”€ Subtotal: $0.045 (was $0.15)                            â”‚
â”‚                                                             â”‚
â”‚  DESIGN (Standard 10x)                                      â”‚
â”‚  â””â”€ Posts to analyze: 25 (was 30)                           â”‚
â”‚  â””â”€ Tokens: ~60K                                            â”‚
â”‚  â””â”€ Cost: $0.18                                             â”‚
â”‚  â””â”€ Subtotal: $0.18 (was $0.23)                             â”‚
â”‚                                                             â”‚
â”‚  DELIVER (Frugal 1x)                                        â”‚
â”‚  â””â”€ File writes: $0                                         â”‚
â”‚  â””â”€ Subtotal: $0.00                                         â”‚
â”‚                                                             â”‚
â”‚  IMPLICIT FEEDBACK (Tier 0) â­ NEW                           â”‚
â”‚  â””â”€ File watcher: $0                                        â”‚
â”‚  â””â”€ Subtotal: $0.00                                         â”‚
â”‚                                                             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚  HOURLY TOTAL: ~$0.23 (was $0.38, 40% ì ˆê°!)                â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.2 Daily & Monthly Projection (v1.1)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COST COMPARISON: v1.0 vs v1.1                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚                        v1.0          v1.1 (Optimized)       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Hourly cost:          $0.38         $0.23                  â”‚
â”‚  Daily (24 cycles):    $9.12         $5.52                  â”‚
â”‚  Retrospective:        $0.50         $0.50                  â”‚
â”‚  Consensus (weekly):   $0.43/day     $0.43/day              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  DAILY TOTAL:          $10.05        $6.45                  â”‚
â”‚  MONTHLY TOTAL:        $301.50       $193.50                â”‚
â”‚                                                             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚  ì ˆê°: $108/month (36% ì ˆê°!)                               â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.3 Further Optimization Options

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ADDITIONAL COST REDUCTION STRATEGIES                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Current (v1.1): $193.50/month                              â”‚
â”‚                                                             â”‚
â”‚  Option A: Stricter Tier 0 (top 20% pass)                   â”‚
â”‚  â””â”€ Monthly: ~$150                                          â”‚
â”‚  â””â”€ Risk: Might miss borderline relevant posts              â”‚
â”‚                                                             â”‚
â”‚  Option B: 2-hour cycle instead of hourly                   â”‚
â”‚  â””â”€ Monthly: ~$100                                          â”‚
â”‚  â””â”€ Trade-off: Less real-time, but sufficient for most      â”‚
â”‚                                                             â”‚
â”‚  Option C: Night mode (4h cycle 00:00-08:00)                â”‚
â”‚  â””â”€ Monthly: ~$160                                          â”‚
â”‚  â””â”€ Trade-off: Reduced coverage during sleep hours          â”‚
â”‚                                                             â”‚
â”‚  Option D: All optimizations combined                       â”‚
â”‚  â””â”€ Monthly: ~$50-70                                        â”‚
â”‚  â””â”€ Best for: Budget-conscious users                        â”‚
â”‚                                                             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚  AGGRESSIVE MODE: $50/month = $1.67/day                     â”‚
â”‚  BALANCED MODE: $100/month = $3.33/day                      â”‚
â”‚  FULL MODE: $193/month = $6.45/day                          â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.4 Value vs Cost (Updated)

```
Monthly Cost Range: $50 ~ $193
Monthly Value:
  - 250~750 curated insights (depends on mode)
  - Zero manual browsing time: ~20-40 hours saved
  - Effective hourly rate: $2.50-5.00/hour of saved time
  
  + Serendipitous discoveries
  + Growing personalized knowledge base
  + Evolving interest ontology
  + ZERO friction feedback (just use Obsidian normally!)
```

---

## 10. Deployment

### 10.1 Local Development

```bash
# 1. Clone and setup
git clone https://github.com/yourname/reddit-second-brain
cd reddit-second-brain

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate

# 3. Install dependencies
pip install -e ".[dev]"

# 4. Configure
cp .env.example .env
# Edit .env with your Reddit API credentials

# 5. Initialize
python -m reddit_brain init

# 6. Run single cycle (test)
python -m reddit_brain cycle

# 7. Start 24/7 operation
python -m reddit_brain start
```

### 10.2 Docker Deployment

```dockerfile
# Dockerfile

FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

# Healthcheck
HEALTHCHECK --interval=5m --timeout=3s \
  CMD python -m reddit_brain health || exit 1

CMD ["python", "-m", "reddit_brain", "start"]
```

```yaml
# docker-compose.yml

version: "3.8"

services:
  reddit-brain:
    build: .
    restart: unless-stopped
    env_file:
      - .env
    volumes:
      - ./data:/app/data
      - /path/to/obsidian/vault:/vault
    environment:
      - OUROBOROS_CHECKPOINT_PATH=/app/data/checkpoints
      - BRAIN_VAULT_PATH=/vault
```

### 10.3 Cloud Deployment (Railway/Fly.io)

```toml
# fly.toml

app = "reddit-second-brain"
primary_region = "nrt"  # Tokyo

[build]
  dockerfile = "Dockerfile"

[env]
  OUROBOROS_MODE = "production"

[mounts]
  source = "brain_data"
  destination = "/app/data"

[[services]]
  internal_port = 8080
  protocol = "tcp"

  [[services.http_checks]]
    interval = "5m"
    timeout = "2s"
    path = "/health"
```

---

## Appendix A: Glossary

| Term | Definition |
|------|------------|
| **Second Brain** | ê°œì¸ ì§€ì‹ ê´€ë¦¬ ì‹œìŠ¤í…œ (Obsidian, Notion ë“±) |
| **Ontology** | ê´€ì‹¬ì‚¬, í‚¤ì›Œë“œ, ê´€ê³„ì˜ êµ¬ì¡°í™”ëœ í‘œí˜„ |
| **Relevance Score** | ê¸€ì´ ì‚¬ìš©ì ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” ì •ë„ (0-1) |
| **Crawl Cycle** | ìˆ˜ì§‘â†’í•„í„°â†’ë¶„ì„â†’ì €ì¥ì˜ í•œ ì‚¬ì´í´ |
| **Retrospective** | ì¼ì¼ ìê¸° ë¶„ì„ ë° ê°œì„  í”„ë¡œì„¸ìŠ¤ |

---

## Appendix B: Troubleshooting

| Issue | Cause | Solution |
|-------|-------|----------|
| No new posts | API rate limit | Check rate limit config |
| Low precision | Ontology too broad | Add more exclude patterns |
| High cost | Too frequent cycles | Reduce to 2h intervals |
| Stagnation | Same content patterns | Let Lateral Thinking explore |

---

*Built with Ouroboros v0.4 â€” "Frugal by Default, Rigorous in Verification"*
